{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ,\n",
    "                'N': wordnet.NOUN,\n",
    "                'V': wordnet.VERB,\n",
    "                'R': wordnet.ADV }\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    # 處理 ' - 符號和轉小寫\n",
    "    sentence = re.sub(r'[\\'-]', '', sentence).lower()\n",
    "    # 只留下英文字母\n",
    "    sentence = re.sub(r'[^a-z]', ' ', sentence)\n",
    "    # tokenize\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # lemmatize\n",
    "    review = []\n",
    "    for word in words:\n",
    "        review.append(lemmatizer.lemmatize(word, get_pos(word)))\n",
    "    \n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604536621c1a4d42a3cd30938a7c6cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b99bfdc8fd9468ebe412f5f9bc5b70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "path = r'.\\aclImdb\\train'\n",
    "folders = ['neg', 'pos']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vocabs = set()\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    for txt in tqdm_notebook(os.listdir(folder_path)):\n",
    "        with open(os.path.join(folder_path, txt), mode='r',encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            vocabs = vocabs.union(set(preprocessing(content)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 78602\n",
      "vocab length after removing stopwords: 78468\n"
     ]
    }
   ],
   "source": [
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocabs)}\")\n",
    "\n",
    "vocabs = vocabs - set(stopwords.words('english'))\n",
    "\n",
    "print(f\"vocab length after removing stopwords: {len(vocabs)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "vocabs = list(vocabs)\n",
    "vocab_dict = {}\n",
    "\n",
    "index = 0\n",
    "for v in vocabs:\n",
    "    if v not in vocab_dict:\n",
    "        vocab_dict[v] = index\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.\\\\aclImdb\\\\train\\\\neg\\\\0_3.txt', 0), ('.\\\\aclImdb\\\\train\\\\neg\\\\10000_4.txt', 0)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "review_pairs = []\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    for txt in os.listdir(folder_path):\n",
    "        review_pairs.append((os.path.join(folder_path, txt), i))\n",
    "\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    with open(review_path, mode='r', encoding='utf-8') as f:\n",
    "        sentence = f.read()\n",
    "        reviews = preprocessing(sentence)\n",
    "    \n",
    "    review = list(set(reviews) - set(stopwords.words('english')))\n",
    "    \n",
    "    return review\n",
    "    \n",
    "\n",
    "def generate_vec(review, vocab_dict):\n",
    "    vec = []\n",
    "    for word in review:\n",
    "        if word in vocab_dict:\n",
    "            vec.append(vocab_dict[word])\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_pairs, vocab_dict):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.vocab = vocab_dict\n",
    "#        self.maxLengh = self.__maxLength__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        review_path = load_review(self.data_pairs[idx][0])\n",
    "        x = generate_vec(review_path, self.vocab)\n",
    "        y = self.data_pairs[idx][1]\n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "#     def __maxLength__(self):\n",
    "#         maxLengh = 0\n",
    "#         for idx in range(self.__len__):\n",
    "#             x, y = self.__getitem__(idx)\n",
    "#             if maxLengh < len(x):\n",
    "#                 maxLengh = len(x)\n",
    "        \n",
    "#         return maxLengh\n",
    "            \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "    length = []\n",
    "    label = []\n",
    "    data = []\n",
    "    outBatch = []\n",
    "    for b in batch:\n",
    "        length.append(len(b[0]))\n",
    "        label.append(b[1])\n",
    "    \n",
    "    s = max(length)\n",
    "    \n",
    "    for i, b in enumerate(batch):\n",
    "        data.append(b[0]+[0]*(s-length[i]))\n",
    "    \n",
    "    return (torch.tensor(data), torch.tensor(label), torch.tensor(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[72473, 37316, 15350, 10618, 26508, 27765, 35492, 52941, 39033,  4550,\n",
       "          23411, 71684, 72556, 38173, 20877, 57344,  3525, 28037, 66918, 19344,\n",
       "          23196, 53982, 26205, 75119, 48560, 75298,  8146,  2468, 23245, 19006,\n",
       "          15368, 26483, 55965, 75994, 52590, 54556, 64647, 15297, 78207, 77343,\n",
       "          31341, 19323,  4694, 20161, 52875,  2297, 17837, 74949,  3468, 75930,\n",
       "          58316, 72153, 57295, 22199, 33183,  8473, 71861, 54143, 26273, 60072,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0],\n",
       "         [69022, 25015, 60276,  9001, 68439, 14851, 66107, 69326, 32171, 20276,\n",
       "          45033, 48562, 29162, 30655, 34168, 47290, 28869, 66443, 19361, 14708,\n",
       "           9026, 71886, 15297,  5011, 12474, 65830,  5154, 48726, 20758, 42888,\n",
       "          58879, 37899, 62248, 46433, 49793, 44300, 38753, 26103, 26273, 61384,\n",
       "          13168, 37272, 55249, 41077, 40328, 46159, 33119,  4714, 72065, 26602,\n",
       "          59527, 67286, 60654, 37139, 32657, 51231,  3173, 50611, 56770, 43067,\n",
       "          17273, 58144,  9512, 50617, 52998, 74858, 59379, 34840, 37148, 67605,\n",
       "          52875, 76072, 16682, 15500, 53153, 63738, 37609, 73327, 50819, 34372,\n",
       "          48616, 46035, 64196, 21130, 28037, 38312, 21564, 20800, 20213, 71801,\n",
       "          48939, 37386,  8343,  3217,   104,  3046, 45713, 68827, 22025,  5657,\n",
       "          18486, 32259, 66689, 42806,  8669, 73188, 54096, 21452, 24468, 33183,\n",
       "          11096, 36438,   748,  9883, 39449, 26508, 27765,  8218,  9283, 71684,\n",
       "          36021, 53817, 35235, 13097, 12917, 54425, 29598, 39939, 15245, 19006,\n",
       "          11279, 56556, 24178, 66228, 55965, 30906, 62668, 17756,  5251, 51311,\n",
       "          55357, 48532, 39022, 78148,  5558, 22979, 30637, 37074, 54143, 22829,\n",
       "           2330]]), tensor([0, 0]), tensor([ 60, 151]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "custom_dst = dataset(review_pairs, vocab_dict)\n",
    "custom_dataloader = DataLoader(dataset=custom_dst, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
