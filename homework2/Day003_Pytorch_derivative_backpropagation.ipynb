{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxtArZ7u16sr"
   },
   "source": [
    "### 作業目標: 使用Pytorch進行微分與倒傳遞\n",
    "這份作業我們會實作微分與倒傳遞以及使用Pytorch的Autograd。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_BwC7sg16ss"
   },
   "source": [
    "### 使用Pytorch實作微分與倒傳遞\n",
    "\n",
    "這裡我們很簡單的實作兩層的神經網路進行回歸問題，其中loss function為L2 loss\n",
    "\n",
    "$$\n",
    "L2\\_loss = (y_{pred}-y)^2\n",
    "$$\n",
    "\n",
    "兩層經網路如下所示\n",
    "$$\n",
    "y_{pred} = ReLU(XW_1)W_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ocsA8ch-16st"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機生成x, y\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=500, n_features=1000, n_targets=10, noise=10, random_state=0)\n",
    "X = torch.tensor(X, device=device)\n",
    "y = torch.tensor(y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o2v8hkG616sz",
    "outputId": "0b737d18-59c2-4bb7-f541-e0ca6ab51a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 504346464.0\n",
      "1000 46179.402\n",
      "2000 14213.812\n",
      "3000 4695.682\n",
      "4000 1601.843\n",
      "5000 585.205\n",
      "6000 244.785\n",
      "7000 118.514\n",
      "8000 65.496\n",
      "9000 40.616\n",
      "10000 27.304\n",
      "11000 19.479\n",
      "12000 14.555\n",
      "13000 11.159\n",
      "14000 8.584\n",
      "15000 6.561\n",
      "16000 5.006\n",
      "17000 3.854\n",
      "18000 3.021\n",
      "19000 2.405\n",
      "20000 1.917\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "q = len(X)//N\n",
    "r = len(X)%N\n",
    "batchs = q if r == 0 else q + 1\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "W1 = torch.randn((D_in, H), device=device).double()\n",
    "W2 = torch.randn((H, D_out), device=device).double()\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(20001):\n",
    "  \n",
    "    loss = torch.zeros((1,), device=device)\n",
    "    for batch in range(batchs):\n",
    "        # 向前傳遞: 計算y_pred\n",
    "        X_input = X[batch*N:] if batch == batchs-1 else X[batch*N:batch*N+N]\n",
    "        A = X_input@W1\n",
    "        B = relu(A)\n",
    "        y_pred = B@W2\n",
    "        \n",
    "        diff = y_pred - y[batch*N:]if batch == batchs-1 else y_pred - y[batch*N:batch*N+N]\n",
    "        \n",
    "      # 計算loss\n",
    "        loss += torch.sum(diff*diff)\n",
    "        \n",
    "      # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "        grad_Y = 2.0 * diff\n",
    "        grad_W2 = B.t() @ grad_Y\n",
    "        grad_relu = grad_Y @ W2.t()\n",
    "        grad_relu[A<0] = 0\n",
    "        grad_W1 = X_input.t() @ grad_relu\n",
    "\n",
    "      # 參數更新\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        W2 -= learning_rate * grad_W2\n",
    "    if t%1000 == 0:\n",
    "        print(t, round(loss.item(), ndigits=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9XiShaU16s3"
   },
   "source": [
    "### 使用Pytorch的Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_VP1YW7516s4"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dlj3NwsP16s6",
    "outputId": "0463fd34-3edf-4516-9d36-c1143463790d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 598108224.0\n",
      "1000 49969.031\n",
      "2000 18124.059\n",
      "3000 5504.642\n",
      "4000 1795.553\n",
      "5000 608.606\n",
      "6000 294.78\n",
      "7000 175.359\n",
      "8000 117.449\n",
      "9000 120.748\n",
      "10000 5688.556\n",
      "11000 146.528\n",
      "12000 35.365\n",
      "13000 25.022\n",
      "14000 18.32\n",
      "15000 13.921\n",
      "16000 10.733\n",
      "17000 8.104\n",
      "18000 5.933\n",
      "19000 4.305\n",
      "20000 3.2\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "###<your code>###\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "W1 = torch.randn(D_in, H, device=device, requires_grad=True).double()\n",
    "W2 = torch.randn(H, D_out, device=device, requires_grad=True).double()\n",
    "\n",
    "# 設置learning rate\n",
    "#learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(20001):\n",
    "    total_loss = torch.zeros((1,), device=device)\n",
    "    for batch in range(batchs):\n",
    "        # 向前傳遞: 計算y_pred\n",
    "        X_input = X[batch*N:] if batch == batchs - 1 else X[batch*N:batch*N+N]\n",
    "        y_pred = (relu(X_input@W1))@W2\n",
    "        #y_pred = X_input.mm(W1).clamp(min=0).mm(W2)\n",
    "        \n",
    "        diff = y_pred - y[batch*N:] if batch == batchs - 1 else y_pred - y[batch*N:batch*N+N]\n",
    "        \n",
    "        # 計算loss\n",
    "        loss = torch.sum(diff*diff)\n",
    "        #loss = diff.pow(2).sum()\n",
    "        total_loss += loss\n",
    "        # 呼叫 backward() 之前要先呼叫 retain_grad()，否則 grad 會是 None\n",
    "        W1.retain_grad()\n",
    "        W2.retain_grad()\n",
    "        \n",
    "        # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\n",
    "        with torch.no_grad():\n",
    "        # 更新參數W1 W2\n",
    "            W1 -= learning_rate * W1.grad\n",
    "            W2 -= learning_rate * W2.grad\n",
    "\n",
    "        # 將紀錄的gradient清空(因為已經更新參數)\n",
    "            W1.grad.zero_()\n",
    "            W2.grad.zero_()\n",
    "  \n",
    "    if t % 1000 == 0:\n",
    "        print(t, round(total_loss.item(), ndigits=3))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znJFnEdr16s9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "微分與倒傳遞作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
