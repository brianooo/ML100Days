{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 使用樹型模型進行文章分類\n",
    "\n",
    "本次作業主利用[Amazon Review data中的All Beauty](https://nijianmo.github.io/amazon/index.html)來進行review評價分類(文章分類)\n",
    "\n",
    "資料中將review分為1,2,3,4,5分，而在這份作業，我們將評論改分為差評價、普通評價、優良評價(1,2-->1差評、3-->2普通評價、4,5-->3優良評價)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理\n",
    "文本資料較為龐大，這裡我們取前10000筆資料來進行作業練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 1.0,\n",
       " 'verified': True,\n",
       " 'reviewTime': '02 19, 2015',\n",
       " 'reviewerID': 'A1V6B6TNIC10QE',\n",
       " 'asin': '0143026860',\n",
       " 'reviewerName': 'theodore j bigham',\n",
       " 'reviewText': 'great',\n",
       " 'summary': 'One Star',\n",
       " 'unixReviewTime': 1424304000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load json data\n",
    "N = 10000\n",
    "all_reviews = []\n",
    "###<your code>###\n",
    "\n",
    "count = 0\n",
    "with open('All_Beauty.json') as file:\n",
    "    for review in file.readlines():\n",
    "        if count < N:\n",
    "            all_reviews.append(json.loads(review))\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "<class 'str'>\n",
      "9995 9995\n",
      "[1, 3, 3, 3, 3, 3, 3, 1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "#parse label(overall) and corpus(reviewText)\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "###<your code>###\n",
    "for review in all_reviews:\n",
    "    if review.get('overall') == None or review.get('reviewText') == None:\n",
    "        continue\n",
    "    else:\n",
    "        labels.append(review['overall'])\n",
    "        corpus.append(review['reviewText'])\n",
    "\n",
    "print(type(labels[0]))\n",
    "print(type(corpus[0]))\n",
    "#transform labels: 1,2 --> 1 and 3 --> 2 and 4,5 --> 3\n",
    "\n",
    "###<your code>###\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     if labels[i] < 3:\n",
    "#         labels[i] = 1\n",
    "#     elif labels[i] == 3:\n",
    "#         labels[i] = 2\n",
    "#     else:\n",
    "#         labels[i] = 3\n",
    "\n",
    "def mapping(x):\n",
    "    if x < 3:\n",
    "        return 1\n",
    "    elif x == 3:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "labels = list(map(mapping, labels))\n",
    "\n",
    "print(len(labels), len(corpus))\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "\n",
      "My  husband wanted to reading about the Negro Baseball and this a great addition to his library\n",
      " Our library doesn't haveinformation so this book is his start. Tthank you\n",
      "\n",
      "This book was very informative, covering all aspects of game.\n",
      "\n",
      "I am already a baseball fan and knew a bit about the Negro leagues, but I learned a lot more reading this book.\n",
      "\n",
      "This was a good story of the Black leagues. I bought the book to teach in my high school reading class. I found it very informative and exciting. I would recommend to anyone interested in the history of the black leagues. It is well written, unlike a book of facts. The McKissack's continue to write good books for young audiences that can also be enjoyed by adults!\n",
      "\n",
      "Today I gave a book about the Negro Leagues of Baseball to a traveling friend. Its a book I've read more than once and felt that my friend would truly enjoy. It felt like giving a gift that you wanted to keep for yourself. I parted with the book knowing that my friend would enjoy reading it on his journey back east. Before giving him the book I spent about thirty minutes flipping through its pages and saying goodbye to some of the stories in it. I know I'll come across the book again, but for me, parting with books is like wishing a friend well on a journey, just as this friend of mine was journeying. Its great to send a visiting friend off with a friendly gift.\n",
      "\n",
      "Well, in leafing through the book's pages I came across a few paragraphs I wanted to retain as a memory of my friend the book. Here below are the book and the lines from it that show in words what the negro baseball players faced every day of their lives; a color barrier that prevented them from gaining national sports recognition as professionals of the game they loved so much, baseball. Though Negro men had proven themselves heroic, capable of soldiering bravely in foreign battles of World War II, America was still treating them as second class citizens or less here at home. Major League Baseball would lead the nation in recovering from its racial prejudicial past. The nation today, even with a Black President, is still playing catch up.\n",
      "\n",
      "excerpt from Black Diamond:  \"Only one thing is keeping them out of the big leagues, and that is the pigmentation of their skin.\" Shirley Povich washington post\n",
      "\n",
      "The story of race relations in American history is one of lost opportunity.  This concise history of the Negro Baseball Leagues for young adult readers illustrates this basic point.  The book discusses the names, dates, and circumstances of the major figures and events of the flip side of American baseball history. Legendary names such as Josh Gibson, \"Cool Papa\" Bell, and Satchel Page are just a few of the remarkable players who made important contributions to the game.  Among others, they played for teams with names such as The Homestead Grays, The St. Louis Stars, and The Kansas City Monarchs.  The book also tells the story of owners and managers, such names as Negro National League founder Rube Foster and the tough as nails woman owner of the Newark Eagles, Effa Manley spring to mind.  The book also briefly explains how the infamous Jim Crow tradition brought about the Negro Baseball saga.  In 1947, major league baseball was sucessfully integrated and that spelled the doom of the Negro Baseball leagues.  The text includes an ample number of photographs.  To assist younger readers, and for easy reference purposes, a player profile and time line section is provided at the back of the book.  Underlying the historical text, there is the theme that segregated baseball mirrored the nagging problem of racism in America.  A sense of fair play and even-handed justice demands that talent, skill, and just plain style should be celebrated, regardless of race.  To do otherwise cheats everybody of a rewarding experience.  Imagine if the great players of the Negro Baseball Leagues had the chance to play (with or against) the great players of the major leagues.  Consider the void created by lost opportunity.  ;-)\n",
      "\n",
      "I didn't like this product it smudged all under my eyes throughly the day\n",
      "\n",
      "I simply love the product. I appreciate print feed back regarding my order\n",
      "\n",
      "it burns your eyes when u put it on  and very light so u have to keep going back n forth a lot to get the dark eyeliner color.\n",
      "also it smudges lot. waste of money.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]:\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#preprocessing data\n",
    "#remove email address, punctuations, and change line symbol(\\n)\n",
    "\n",
    "###<your code>###\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_word_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_mapping = {'J': wordnet.ADJ,\n",
    "                   'N': wordnet.NOUN,\n",
    "                   'V': wordnet.VERB,\n",
    "                   'R': wordnet.ADV}\n",
    "    return tag_mapping.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_content(X):\n",
    "    # remove non-alphabet but ' - , then transfer to lowercase\n",
    "    X_remove_symbol = [re.sub('[^a-zA-Z\\'\\-]', ' ', x).lower() for x in X]\n",
    "    \n",
    "    # word tokenize\n",
    "    X_tokenize = [nltk.word_tokenize(x) for x in X_remove_symbol]\n",
    "    \n",
    "    # remove stopwords then lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    X_lemmatize = []\n",
    "    for content in X_tokenize:\n",
    "        content_clean = []\n",
    "        for word in content:\n",
    "            if word not in stop_words:\n",
    "                word = lemmatizer.lemmatize(word, get_word_pos(word))\n",
    "                content_clean.append(word)\n",
    "        X_lemmatize.append(content_clean)\n",
    "    \n",
    "    X_output = [' '.join(x) for x in X_lemmatize]\n",
    "    \n",
    "    return X_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7996, 1999, 7996, 1999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split corpus and label into train and test\n",
    "###<your code>###\n",
    "X = clean_content(corpus)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=2)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7844"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 確認總共有幾個字\n",
    "def how_many_words(X: list) -> int:\n",
    "    total = set()\n",
    "    for x in X:\n",
    "        content = x.split(' ')\n",
    "        total |= set(content)\n",
    "    return len(total)\n",
    "\n",
    "how_many_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change corpus into vector\n",
    "#you can use tfidf or BoW here\n",
    "\n",
    "###<your code>###\n",
    "\n",
    "cv = CountVectorizer(max_features=2000)\n",
    "\n",
    "#transform training and testing corpus into vector form\n",
    "x_train_cv = cv.fit_transform(X_train).toarray()\n",
    "x_test_cv = cv.transform(X_test).toarray()\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2000, smooth_idf=True)\n",
    "\n",
    "#transform training and testing corpus into vector form\n",
    "x_train_tf = tfidf.fit_transform(X_train).toarray()\n",
    "x_test_tf = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishing 501/7996\n",
      "finishing 1001/7996\n",
      "finishing 1501/7996\n",
      "finishing 2001/7996\n",
      "finishing 2501/7996\n",
      "finishing 3001/7996\n",
      "finishing 3501/7996\n",
      "finishing 4001/7996\n",
      "finishing 4501/7996\n",
      "finishing 5001/7996\n",
      "finishing 5501/7996\n",
      "finishing 6001/7996\n",
      "finishing 6501/7996\n",
      "finishing 7001/7996\n",
      "finishing 7501/7996\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 共現矩陣 --> PPMI --> SVD 降維 --> 計算文本向量\n",
    "import numpy as np\n",
    "# 共現矩陣\n",
    "def create_co_matrix(corpus: list, vocab_list: list, word2idx: dict,\n",
    "                     window_size: int=1, use_weighting: bool=False, verbose: bool=False) -> np.ndarray:\n",
    "    '''Function to create co-occurrence matrix\n",
    "    '''\n",
    "    #initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(len(word2idx), len(word2idx)), dtype=np.int32)\n",
    "    \n",
    "    for idx, sms in enumerate(corpus):\n",
    "        sms_list = sms.split(' ')\n",
    "        sms_ids = list()\n",
    "        \n",
    "        for w in sms_list:\n",
    "            if w in word2idx:                                 # 將每一筆資料轉成 word index\n",
    "                sms_ids.append(word2idx[w])                   # 若不在 word index 裡的字都轉成 -1\n",
    "            else:\n",
    "                sms_ids.append(-1)\n",
    "        \n",
    "        for center_i, center_word_id in enumerate(sms_ids):\n",
    "            if center_word_id != -1:\n",
    "                left_center_i = center_i - window_size if center_i-window_size >= 0 else 0\n",
    "                context_ids = sms_ids[left_center_i:center_i]\n",
    "\n",
    "                for left_i, left_word_id in enumerate(context_ids):\n",
    "                    if left_word_id != -1:\n",
    "                        co_matrix[center_word_id, left_word_id] += 1\n",
    "                        co_matrix[left_word_id, center_word_id] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            if idx != 0 and idx%500 == 0:\n",
    "                    print(f\"finishing {idx+1}/{len(corpus)}\")\n",
    "    print(\"Done\")\n",
    "    if use_weighting:\n",
    "        # if use weighting, then we set the co-occurrence with the word itself to 1.0\n",
    "        for i in range(len(word2idx)):\n",
    "            co_matrix[i, i] = 1\n",
    "        \n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(X_train, tfidf.get_feature_names(), tfidf.vocabulary_,\n",
    "                            window_size=3, use_weighting=True, verbose=True)\n",
    "\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log2\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.154468  , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 5.9564476 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.55104226, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 7.4699054 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 7.141132  ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        7.5200877 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPMI\n",
    "\n",
    "def ppmi(co_matrix: np.ndarray, eps: float=1e-8, verbose: bool=False):\n",
    "    import numpy as np\n",
    "    M = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "    N = np.sum(co_matrix)\n",
    "    S = np.sum(co_matrix, axis=0)\n",
    "    total = co_matrix.shape[0]*co_matrix.shape[1]\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            pmi = np.log2(co_matrix[i, j]*N / (S[i]*S[j]+eps))\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % 10 == 0 or cnt == total:\n",
    "                    print(f\"{cnt}/{total} Done\")\n",
    "    \n",
    "    return M\n",
    "\n",
    "ppmi_matrix = ppmi(co_matrix, verbose=False)\n",
    "ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components = 50 and explained variance = 0.16709251701831818\n",
      "Number of components = 100 and explained variance = 0.2617502808570862\n",
      "Number of components = 150 and explained variance = 0.33791667222976685\n",
      "Number of components = 200 and explained variance = 0.4043768048286438\n",
      "Number of components = 250 and explained variance = 0.463849276304245\n",
      "Number of components = 300 and explained variance = 0.5179453492164612\n",
      "Number of components = 350 and explained variance = 0.5669411420822144\n",
      "Number of components = 400 and explained variance = 0.6118349432945251\n",
      "Number of components = 450 and explained variance = 0.6525866389274597\n",
      "Number of components = 500 and explained variance = 0.6897177696228027\n",
      "Number of components = 550 and explained variance = 0.7236347198486328\n",
      "Number of components = 600 and explained variance = 0.7544156312942505\n",
      "Number of components = 650 and explained variance = 0.7824419736862183\n",
      "Number of components = 700 and explained variance = 0.8079986572265625\n",
      "Number of components = 750 and explained variance = 0.8311662673950195\n",
      "Number of components = 800 and explained variance = 0.8520738482475281\n",
      "Number of components = 850 and explained variance = 0.8710288405418396\n",
      "Number of components = 900 and explained variance = 0.888054609298706\n",
      "Number of components = 950 and explained variance = 0.9033515453338623\n",
      "Number of components = 1000 and explained variance = 0.9170217514038086\n",
      "Number of components = 1050 and explained variance = 0.9291050434112549\n",
      "Number of components = 1100 and explained variance = 0.9398556351661682\n",
      "Number of components = 1150 and explained variance = 0.9492394924163818\n",
      "Number of components = 1200 and explained variance = 0.9576258659362793\n",
      "Number of components = 1250 and explained variance = 0.9652057886123657\n",
      "Number of components = 1300 and explained variance = 0.9719136953353882\n",
      "Number of components = 1350 and explained variance = 0.9776198267936707\n",
      "Number of components = 1400 and explained variance = 0.9824934005737305\n",
      "Number of components = 1450 and explained variance = 0.986579418182373\n",
      "Number of components = 1500 and explained variance = 0.9899716377258301\n",
      "Number of components = 1550 and explained variance = 0.9927217960357666\n",
      "Number of components = 1600 and explained variance = 0.994902491569519\n",
      "Number of components = 1650 and explained variance = 0.996594250202179\n",
      "Number of components = 1700 and explained variance = 0.9978748559951782\n",
      "Number of components = 1750 and explained variance = 0.9987766742706299\n",
      "Number of components = 1800 and explained variance = 0.9993801116943359\n",
      "Number of components = 1850 and explained variance = 0.9997401237487793\n",
      "Number of components = 1900 and explained variance = 0.9999226927757263\n",
      "Number of components = 1950 and explained variance = 0.9999904632568359\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVD reduce dimension\n",
    "# Program to find the optimal number of components for Truncated SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_comp = range(50,2000,50) # list containing different values of components\n",
    "variance_sum = [] # explained variance ratio for each component of Truncated SVD\n",
    "\n",
    "for dim in n_comp:\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    svd.fit(ppmi_matrix)\n",
    "    variance_sum.append(svd.explained_variance_ratio_.sum())\n",
    "    print(f'Number of components = {dim} and explained variance = {variance_sum[-1]}')\n",
    "    \n",
    "plt.plot(n_comp, variance_sum)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.title(\"Plot of Number of components v/s explained variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce dimension, take dimension=1300\n",
    "\n",
    "svd = TruncatedSVD(n_components=1300)\n",
    "ppmi_transformed = svd.fit_transform(ppmi_matrix)\n",
    "\n",
    "ppmi_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing training:  36\n",
      "Number of missing testing:  15\n"
     ]
    }
   ],
   "source": [
    "# compute vector which represents document\n",
    "# get doc vector via take mean of all word vectors inside the corresponding document\n",
    "\n",
    "def make_doc_vectors(corpus: list, word2idx: dict, vocab_list: list):\n",
    "    \n",
    "    # vectorizing data \n",
    "    # and make document vector by take mean to all word vector\n",
    "    import numpy as np\n",
    "    doc_vec = []\n",
    "    empty_doc_list = []\n",
    "    for i, sms_msg in enumerate(corpus):\n",
    "        sms_msg = [word2idx[word] for word in sms_msg.split() if word in vocab_list] #tokenize\n",
    "        if len(sms_msg) > 0:\n",
    "            sms_msg = np.array([ppmi_transformed[ids] for ids in sms_msg]) #vectorize\n",
    "            doc_vec.append(sms_msg.mean(axis=0))\n",
    "        else:\n",
    "            empty_doc_list.append(i)\n",
    "#             print(f\"document {i} doesn't contain word in vocab_list\")\n",
    "#             print(corpus[i])\n",
    "#             print(\"\\n\")\n",
    "        \n",
    "    return np.vstack(doc_vec), empty_doc_list\n",
    "\n",
    "word2idx = tfidf.vocabulary_\n",
    "vocab_list = tfidf.get_feature_names()\n",
    "\n",
    "x_train_ppmi, missing_train_list = make_doc_vectors(X_train, word2idx, vocab_list)\n",
    "print('Number of missing training: ', len(missing_train_list))\n",
    "\n",
    "x_test_ppmi, missing_test_list = make_doc_vectors(X_test, word2idx, vocab_list)\n",
    "print('Number of missing testing: ', len(missing_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing terms\n",
    "# y_train\n",
    "y_train_filter = np.delete(np.array(y_train), missing_train_list)\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(doc_vec_train, y_train_filter)\n",
    "\n",
    "# y_test\n",
    "y_test_filter = np.delete(np.array(y_test), missing_test_list)\n",
    "# train_pred = knn.predict(doc_vec_train)\n",
    "# test_pred = knn.predict(doc_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練與預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Bag of Words --------------------\n",
      "Accuracy: 0.9044522261130565\n",
      "Depth: 10, Number of leaves: 59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.31      0.42       137\n",
      "           2       0.00      0.00      0.00        76\n",
      "           3       0.91      0.99      0.95      1786\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.52      0.43      0.46      1999\n",
      "weighted avg       0.86      0.90      0.88      1999\n",
      "\n",
      "[[  42    0   95]\n",
      " [   4    0   72]\n",
      " [  18    2 1766]]\n",
      "-------------------- TFIDF --------------------\n",
      "Accuracy: 0.9034517258629314\n",
      "Depth: 10, Number of leaves: 42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.22      0.33       137\n",
      "           2       0.00      0.00      0.00        76\n",
      "           3       0.91      0.99      0.95      1786\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.54      0.40      0.43      1999\n",
      "weighted avg       0.86      0.90      0.87      1999\n",
      "\n",
      "[[  30    0  107]\n",
      " [   3    0   73]\n",
      " [  10    0 1776]]\n",
      "-------------------- PPMI --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8785282258064516\n",
      "Depth: 10, Number of leaves: 156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.29      0.34       137\n",
      "           2       0.03      0.01      0.02        75\n",
      "           3       0.92      0.96      0.94      1772\n",
      "\n",
      "    accuracy                           0.88      1984\n",
      "   macro avg       0.45      0.42      0.43      1984\n",
      "weighted avg       0.85      0.88      0.86      1984\n",
      "\n",
      "[[  40    7   90]\n",
      " [   9    1   65]\n",
      " [  48   22 1702]]\n"
     ]
    }
   ],
   "source": [
    "#build classification model (decision tree, random forest, or adaboost)\n",
    "#start training\n",
    "# Decision Tree\n",
    "def DT(x_train, x_test, y_train, y_test):\n",
    "    dt = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=20, min_samples_leaf=10)\n",
    "    dt.fit(x_train, y_train)\n",
    "\n",
    "    # 計算準確度\n",
    "    print(f\"Accuracy: {dt.score(x_test,y_test)}\")\n",
    "\n",
    "    # 查看決策樹深度與終端節點個數\n",
    "    print(f\"Depth: {dt.get_depth()}, Number of leaves: {dt.get_n_leaves()}\")\n",
    "    \n",
    "    #calculate confusion matrix, precision, recall, and f1-score\n",
    "    y_pred = dt.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*20, 'Bag of Words', '-'*20)\n",
    "DT(x_train_cv, x_test_cv, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'TFIDF', '-'*20)\n",
    "DT(x_train_tf, x_test_tf, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'PPMI', '-'*20)\n",
    "DT(x_train_ppmi, x_test_ppmi, y_train_filter, y_test_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上述資訊可以發現, 模型在好評的準確度高(precision, recall都高), 而在差評的部分表現較不理想, 在普通評價的部分大部分跟差評搞混,\n",
    "同學可以試著學習到的各種方法來提升模型的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Bag of Words --------------------\n",
      "Accuracy: 0.8934467233616809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       137\n",
      "           2       0.00      0.00      0.00        76\n",
      "           3       0.89      1.00      0.94      1786\n",
      "\n",
      "    accuracy                           0.89      1999\n",
      "   macro avg       0.30      0.33      0.31      1999\n",
      "weighted avg       0.80      0.89      0.84      1999\n",
      "\n",
      "[[   0    0  137]\n",
      " [   0    0   76]\n",
      " [   0    0 1786]]\n",
      "-------------------- TFIDF --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8934467233616809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       137\n",
      "           2       0.00      0.00      0.00        76\n",
      "           3       0.89      1.00      0.94      1786\n",
      "\n",
      "    accuracy                           0.89      1999\n",
      "   macro avg       0.30      0.33      0.31      1999\n",
      "weighted avg       0.80      0.89      0.84      1999\n",
      "\n",
      "[[   0    0  137]\n",
      " [   0    0   76]\n",
      " [   0    0 1786]]\n",
      "-------------------- PPMI --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8936491935483871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.01      0.01       137\n",
      "           2       0.00      0.00      0.00        75\n",
      "           3       0.89      1.00      0.94      1772\n",
      "\n",
      "    accuracy                           0.89      1984\n",
      "   macro avg       0.63      0.34      0.32      1984\n",
      "weighted avg       0.87      0.89      0.84      1984\n",
      "\n",
      "[[   1    0  136]\n",
      " [   0    0   75]\n",
      " [   0    0 1772]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "def RF(x_train, x_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=50, criterion='entropy', max_depth=10, min_samples_split=20, min_samples_leaf=10)\n",
    "    rf.fit(x_train, y_train)\n",
    "\n",
    "    # 計算準確度\n",
    "    print(f\"Accuracy: {rf.score(x_test,y_test)}\")\n",
    "    \n",
    "    #calculate confusion matrix, precision, recall, and f1-score\n",
    "    y_pred = rf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*20, 'Bag of Words', '-'*20)\n",
    "RF(x_train_cv, x_test_cv, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'TFIDF', '-'*20)\n",
    "RF(x_train_tf, x_test_tf, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'PPMI', '-'*20)\n",
    "RF(x_train_ppmi, x_test_ppmi, y_train_filter, y_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Bag of Words --------------------\n",
      "Accuracy: 0.8979489744872436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.28      0.39       137\n",
      "           2       0.24      0.07      0.10        76\n",
      "           3       0.92      0.98      0.95      1786\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.58      0.44      0.48      1999\n",
      "weighted avg       0.87      0.90      0.88      1999\n",
      "\n",
      "[[  39    2   96]\n",
      " [   5    5   66]\n",
      " [  21   14 1751]]\n",
      "-------------------- TFIDF --------------------\n",
      "Accuracy: 0.8864432216108054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.18      0.26       137\n",
      "           2       0.10      0.03      0.04        76\n",
      "           3       0.91      0.98      0.94      1786\n",
      "\n",
      "    accuracy                           0.89      1999\n",
      "   macro avg       0.48      0.40      0.41      1999\n",
      "weighted avg       0.85      0.89      0.86      1999\n",
      "\n",
      "[[  25    6  106]\n",
      " [   2    2   72]\n",
      " [  29   12 1745]]\n",
      "-------------------- PPMI --------------------\n",
      "Accuracy: 0.891633064516129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.04      0.08       137\n",
      "           2       0.00      0.00      0.00        75\n",
      "           3       0.90      0.99      0.94      1772\n",
      "\n",
      "    accuracy                           0.89      1984\n",
      "   macro avg       0.47      0.35      0.34      1984\n",
      "weighted avg       0.83      0.89      0.85      1984\n",
      "\n",
      "[[   6    0  131]\n",
      " [   0    0   75]\n",
      " [   6    3 1763]]\n"
     ]
    }
   ],
   "source": [
    "# Adaboost\n",
    "def Adaboost(x_train, x_test, y_train, y_test):\n",
    "    adbt = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',\n",
    "                                                                            max_depth=10,\n",
    "                                                                            min_samples_split=20,\n",
    "                                                                            min_samples_leaf=10),\n",
    "                                      n_estimators=50,\n",
    "                                      learning_rate=0.8)\n",
    "\n",
    "    #使用adaboost模型進行訓練\n",
    "    adbt.fit(x_train, y_train)\n",
    "\n",
    "    # 計算準確度\n",
    "    print(f\"Accuracy: {adbt.score(x_test,y_test)}\")\n",
    "    \n",
    "    #calculate confusion matrix, precision, recall, and f1-score\n",
    "    y_pred = adbt.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*20, 'Bag of Words', '-'*20)\n",
    "Adaboost(x_train_cv, x_test_cv, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'TFIDF', '-'*20)\n",
    "Adaboost(x_train_tf, x_test_tf, y_train, y_test)\n",
    "\n",
    "print('-'*20, 'PPMI', '-'*20)\n",
    "Adaboost(x_train_ppmi, x_test_ppmi, y_train_filter, y_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
