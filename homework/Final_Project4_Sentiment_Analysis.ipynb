{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "# 另一個 stopwords 的來源\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "\n",
    "# 讀正向與負向 reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<review_text>\n",
       " I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
       " \n",
       " I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
       " \n",
       " As always, Amazon had it to me in &lt;2 business days\n",
       " </review_text>, <review_text>\n",
       " I ordered 3 APC Back-UPS ES 500s on the recommendation of an employee of mine who used to work at APC. I've had them for about a month now without any problems. They've functioned properly through a few unexpected power interruptions. I'll gladly order more if the need arises.\n",
       " \n",
       " Pros:\n",
       "  - Large plug spacing, good for power adapters\n",
       "  - Simple design\n",
       "  - Long cord\n",
       " \n",
       " Cons:\n",
       "  - No line conditioning (usually an expensive option\n",
       " </review_text>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基於nltk自建 tokenizer\n",
    "\n",
    "def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \n",
    "                \"N\": wordnet.NOUN, \n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet. ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    s = re.sub('[^a-z]', ' ', s)   \n",
    "    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n",
    "    tokens = [t for t in tokens if len(t) > 3] # 去除短字\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t, get_pos(t)) for t in tokens] # 去除大小寫\n",
    "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 7438\n"
     ]
    }
   ],
   "source": [
    "# 先產生 word-to-index map 再產生 word-frequency vectors\n",
    "# 同時儲存 tokenized 版本未來不需再做 tokenization\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # 正規化數據提升未來準確度\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data 創造 train/test splits\n",
    "# 多次嘗試!\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# 最後 100 列是測試用\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8047368421052632\n",
      "Test accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", dt.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", dt.score(Xtest, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8115789473684211\n",
      "Test accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "ada.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", ada.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", ada.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "rf.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", rf.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", rf.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index_to_word map\n",
    "\n",
    "index_to_word = {}\n",
    "for key, value in word_index_map.items():\n",
    "    index_to_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(227, 'return', 0.020806125188699284),\n",
       " (116, 'price', 0.012334816641974507),\n",
       " (1040, 'bad', 0.009347897441354945),\n",
       " (471, 'try', 0.009154045840986247),\n",
       " (1, 'this', 0.008039384042679301),\n",
       " (182, 'highly', 0.007670795725157828),\n",
       " (103, 'perfect', 0.007623075764007412),\n",
       " (1604, 'waste', 0.007410307731764408),\n",
       " (67, 'easy', 0.007379002719783689),\n",
       " (230, 'excellent', 0.007067332880954998),\n",
       " (407, 'poor', 0.00670830051265114),\n",
       " (189, 'support', 0.006612368719582323),\n",
       " (4622, 'refund', 0.0063713277329665394),\n",
       " (461, 'then', 0.0063348895024097),\n",
       " (81, 'quality', 0.006286659246516459),\n",
       " (120, 'money', 0.006045861741864559),\n",
       " (101, 'item', 0.0052201033945255305),\n",
       " (449, 'disappointed', 0.005042908510257112),\n",
       " (109, 'fast', 0.004961898594208305),\n",
       " (207, 'stop', 0.004791310176796104)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_index = list(np.argsort(rf.feature_importances_))\n",
    "sort_index.reverse()\n",
    "\n",
    "sort_importance = []\n",
    "\n",
    "for index in sort_index:\n",
    "    sort_importance.append((index, index_to_word[index], rf.feature_importances_[index]))\n",
    "    \n",
    "sort_importance[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.7538078927145454\n",
      "cable 0.767100494683746\n",
      "time -0.726223384728383\n",
      "month -0.8138183269413682\n",
      "space 0.6225515661287355\n",
      "sound 0.9415876965017499\n",
      "easy 2.237605658770963\n",
      "quality 1.4511810807116838\n",
      "company -0.6567263804005102\n",
      "item -1.0927078510174726\n",
      "perfect 1.2293113909512634\n",
      "fast 1.0859964956223818\n",
      "price 2.9279483326231057\n",
      "value 0.5822351320830279\n",
      "money -1.1176839401008392\n",
      "memory 1.0387507759065526\n",
      "picture 0.6537187302855398\n",
      "happy 0.6073924511806442\n",
      "travel 0.5609180069083064\n",
      "pretty 0.7990758582758279\n",
      "pleased 0.5119059026005125\n",
      "highly 1.2408898316921535\n",
      "recommend 0.8304659260252012\n",
      "customer -0.7753894694335149\n",
      "support -1.042725745634144\n",
      "little 1.1340326920537787\n",
      "stop -0.8081967049624343\n",
      "amaze 0.5083003367541621\n",
      "worth 0.5373256673569853\n",
      "sent -0.5472605966183283\n",
      "return -2.8377019893601494\n",
      "excellent 1.4888815795249368\n",
      "extra 0.502023276829129\n",
      "love 1.2204364589752805\n",
      "video 0.5989701405674107\n",
      "feature 0.5522186957764698\n",
      "software -0.5579373573833781\n",
      "home 0.6335105114296778\n",
      "piece -0.6617659615898692\n",
      "useless -0.5556462063082039\n",
      "week -0.8151194395895055\n",
      "size 0.621636765789134\n",
      "apple -0.5020409214133699\n",
      "laptop 0.594090854191708\n",
      "doesn -0.7395919977117874\n",
      "service -0.5138393867729795\n",
      "poor -0.9323139668307392\n",
      "look 0.9334158852705996\n",
      "expect 0.5709530056769769\n",
      "disappointed -0.6006576831040965\n",
      "then -1.3218521738728466\n",
      "call -0.8175383345915141\n",
      "try -1.2284905141939244\n",
      "start -0.6156994315885005\n",
      "didn -0.5111365834710531\n",
      "static -0.5059415930507954\n",
      "radio -0.5451036257336287\n",
      "replace -0.5963165666475047\n",
      "comfortable 0.7903688658365099\n",
      "haven 0.5460721061909384\n",
      "maybe -0.5394382549442641\n",
      "hour -0.5545233604108344\n",
      "speaker 0.994839743235196\n",
      "cheap -0.5491406167531496\n",
      "warranty -0.6959569188012167\n",
      "bad -0.7764933350950349\n",
      "overall 0.5270845062346075\n",
      "error -0.524516193786817\n",
      "broken -0.5461102185582553\n",
      "junk -0.615541448157164\n",
      "die -0.5049148826798193\n",
      "waste -1.318256837895839\n",
      "send -0.8130638649044146\n",
      "paper 0.6970251985052787\n",
      "terrible -0.5710081614512008\n",
      "refund -0.8137770682857509\n"
     ]
    }
   ],
   "source": [
    "# 列出每個字的正負 weight\n",
    "# 用不同的 threshold values!\n",
    "from future.utils import iteritems\n",
    "\n",
    "indexs = []\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "        indexs.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 12, 36, 45, 64, 67, 81, 82, 101]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9752631578947368\n",
      "Test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 取超出 threshold 的字\n",
    "\n",
    "Xtrain_new = Xtrain[:, indexs]\n",
    "Xtest_new = Xtest[:, indexs]\n",
    "\n",
    "rf_new = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "rf_new.fit(Xtrain_new, Ytrain)\n",
    "print(\"Train accuracy:\", rf_new.score(Xtrain_new, Ytrain))\n",
    "print(\"Test accuracy:\", rf_new.score(Xtest_new, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
